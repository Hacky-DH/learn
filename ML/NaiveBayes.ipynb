{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes\n",
    "$$ \\begin{split} \\mathop{argmax}_{c_k}p(y=c_k|x) &= \\mathop{argmax}_{c_k}p(y=c_k)p(x|y=c_k) \\\\\n",
    "& \\left( due to: p(y=c_k|x) = \\frac{p(y=c_k)p(x|y=c_k)}{p(x)} \\right) \\\\\n",
    "&= \\mathop{argmax}_{c_k}p(y=c_k)\\prod_jp(x^{(j)}|y=c_k) \\end{split} $$\n",
    "Use Maximum Likelihood Estimate(MLE) to evaluate $ p(y=c_k)$ and $ p(x^{(j)}|y=c_k) $ in datasets.\n",
    "$$ \\hat{p}(y=c_k) = \\frac{\\sum_i I(y_i=c_k)}{N} \\\\\n",
    "\\hat{p}(x^{(j)}=a_j|y=c_k) = \\frac{\\sum_i I(x_i^{(j)}=a_j,y=c_k)}{I(y_i=c_k)}\n",
    "$$\n",
    "Bayesian estimation add $ \\lambda $ on numerator and denominator in MLE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes in Scikit-learn\n",
    "Classifiers: GaussianNB, MultinomialNB, BernoulliNB\n",
    "\n",
    "## Documents Classification\n",
    "Use TF-IDF(Term Frequency and Inverse Document Frequency) of term in documents as feature\n",
    "$$ TF-IDF = TF*IDF \\\\\n",
    "TF(t) = \\frac {\\text{Number of times term t appears in a document}}{\\text{Total number of terms in the document}}\\\\\n",
    "IDF(t) = log_e\\frac {\\text{Total number of documents}}{\\text{Number of documents with term t in it + 1}} $$\n",
    "sklearn.feature_extraction.text.TfidfVectorizer(stop_words=stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents=[\n",
    "    'my dog has flea problems help please',\n",
    "    'maybe not take him to dog park stupid',\n",
    "    'my dalmation is so cute I love him',\n",
    "    'stop posting stupid worthless garbage',\n",
    "    'mr licks ate my steak how to stop him',\n",
    "    'quit buying worthlsess dog food stupid',\n",
    "]\n",
    "targets=[0,1,0,1,0,1] # 0 normal, 1 insult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_matrix = vect.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ate', 'buying', 'cute', 'dalmation', 'dog', 'flea', 'food', 'garbage', 'has', 'help', 'him', 'how', 'is', 'licks', 'love', 'maybe', 'mr', 'my', 'not', 'park', 'please', 'posting', 'problems', 'quit', 'so', 'steak', 'stop', 'stupid', 'take', 'to', 'worthless', 'worthlsess']\n"
     ]
    }
   ],
   "source": [
    "print(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'my': 17, 'dog': 4, 'has': 8, 'flea': 5, 'problems': 22, 'help': 9, 'please': 20, 'maybe': 15, 'not': 18, 'take': 28, 'him': 10, 'to': 29, 'park': 19, 'stupid': 27, 'dalmation': 3, 'is': 12, 'so': 24, 'cute': 2, 'love': 14, 'stop': 26, 'posting': 21, 'worthless': 30, 'garbage': 7, 'mr': 16, 'licks': 13, 'ate': 0, 'steak': 25, 'how': 11, 'quit': 23, 'buying': 1, 'worthlsess': 31, 'food': 6}\n"
     ]
    }
   ],
   "source": [
    "print(vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 17)\t0.2836156972830696\n",
      "  (0, 4)\t0.2836156972830696\n",
      "  (0, 8)\t0.40966431929307107\n",
      "  (0, 5)\t0.40966431929307107\n",
      "  (0, 22)\t0.40966431929307107\n",
      "  (0, 9)\t0.40966431929307107\n",
      "  (0, 20)\t0.40966431929307107\n",
      "  (1, 4)\t0.28007245489665356\n",
      "  (1, 15)\t0.4045463374809687\n",
      "  (1, 18)\t0.4045463374809687\n",
      "  (1, 28)\t0.4045463374809687\n",
      "  (1, 10)\t0.28007245489665356\n",
      "  (1, 29)\t0.33173378384997615\n",
      "  (1, 19)\t0.4045463374809687\n",
      "  (1, 27)\t0.28007245489665356\n",
      "  (2, 17)\t0.2836156972830696\n",
      "  (2, 10)\t0.2836156972830696\n",
      "  (2, 3)\t0.40966431929307107\n",
      "  (2, 12)\t0.40966431929307107\n",
      "  (2, 24)\t0.40966431929307107\n",
      "  (2, 2)\t0.40966431929307107\n",
      "  (2, 14)\t0.40966431929307107\n",
      "  (3, 27)\t0.3397724008063106\n",
      "  (3, 26)\t0.4024458035648648\n",
      "  (3, 21)\t0.49077900350475\n",
      "  (3, 30)\t0.49077900350475\n",
      "  (3, 7)\t0.49077900350475\n",
      "  (4, 17)\t0.25617597302407796\n",
      "  (4, 10)\t0.25617597302407796\n",
      "  (4, 29)\t0.30342942826735725\n",
      "  (4, 26)\t0.30342942826735725\n",
      "  (4, 16)\t0.3700294328328553\n",
      "  (4, 13)\t0.3700294328328553\n",
      "  (4, 0)\t0.3700294328328553\n",
      "  (4, 25)\t0.3700294328328553\n",
      "  (4, 11)\t0.3700294328328553\n",
      "  (5, 4)\t0.3109015546880969\n",
      "  (5, 27)\t0.3109015546880969\n",
      "  (5, 23)\t0.44907695514940654\n",
      "  (5, 1)\t0.44907695514940654\n",
      "  (5, 31)\t0.44907695514940654\n",
      "  (5, 6)\t0.44907695514940654\n"
     ]
    }
   ],
   "source": [
    "print(tf_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
